{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b5de47",
   "metadata": {
    "tags": [
     "hide-cell",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%pip install openai nltk ipywidgets numpy requests-cache backoff tiktoken nrclex pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f29fd",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with ChatGPT\n",
    "\n",
    "Sentiment Analysis is sort of like the \"Hello, world!\" of Natural Language Processing (NLP), but luckily for us, it's a bit more fun than just echoing out a string - otherwise this workshop could be a bit bland.\n",
    "\n",
    "This notebook will guide you through analyzing sentiment with ChatGPT and discuss some of the differences between how you can approach this problem with a generative AI like ChatGPT versus how you might have approached this problem in the past.\n",
    "\n",
    "**Note**: For a better learning experience, this notebook purposely hides some implementation details like how interactive widgets are created and certain imports of notebook-specific utilities. Full details are available if you open this notebook in your editor of choice or expand the hidden cells.\n",
    "\n",
    "## What is sentiment analysis?\n",
    "\n",
    "Sentiment Analysis is a way of analyzing some text to determine if it's positive, negative, or neutral.\n",
    "\n",
    "This is the kind of thing that's pretty easy for a human who understands the language the text is written in, but it can be hard for a computer to really understand the underlying meaning behind the text.\n",
    "\n",
    "### Examples\n",
    "\n",
    "1. \"I saw that movie.\" - Neutral\n",
    "2. \"I love that movie.\" - Positive\n",
    "3. \"I hate that movie.\" - Negative\n",
    "\n",
    "## How do we analyze sentiment?\n",
    "\n",
    "We'll start with some housekeeping first by making sure that our dependencies are ready.\n",
    "\n",
    "For this demo, we'll start out by exploring a more traditional approach that uses the Python Natural Language Toolkit (NLTK) and then we'll see how our approach might change when we use ChatGPT via the OpenAI SDK instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f45b4b3",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "\n",
    "First, we'll import the relevant tools we'll be using in the notebook and configure some global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e3c67",
   "metadata": {
    "tags": [
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nltk\n",
    "import openai\n",
    "\n",
    "# download nltk data\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# globals\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TEMPERATURE = 0.2\n",
    "STORY_SAMPLE_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d1782d",
   "metadata": {
    "tags": [
     "hide-cell",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "# this cell focuses on some implemetation details for this notebook\n",
    "# that aren't actually important to the workshop\n",
    "import ipywidgets as pywidgets\n",
    "import requests as request\n",
    "import requests_cache\n",
    "import backoff\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "# configuration widgets\n",
    "from widgets.config import modelDropdown, apiKeyInput, apiKeyUpdateButton, temperatureSlider, sampleSizeSlider, sampleSizeWarningLabel, openAiHeader, hackerNewsHeader\n",
    "\n",
    "# project-specific widgets\n",
    "from widgets.simple import simpleAnalysisWidget\n",
    "from widgets.advanced import advancedAnalysisWidget, configureOpenAi\n",
    "from widgets.tokens import tokenAnalysisWidget, configureModel\n",
    "\n",
    "# project-specific utilities\n",
    "from utils.obfuscate import obfuscateKey\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "# we'll use this session to cache our hacker news api requests\n",
    "REQUEST_CACHE_EXPIRATION_SECONDS = 60 * 15\n",
    "session = requests_cache.CachedSession('hackernews_cache', expire_after=REQUEST_CACHE_EXPIRATION_SECONDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9faba98",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The code cell below renders a configuration form that you can use to adjust some variables used by other cells in this notebook.\n",
    "\n",
    "You can make changes to the configuration form at any time and rerun cells that make requests to the OpenAI API or Hacker News API to see how the results change.\n",
    "\n",
    "You can configure the following values:\n",
    "\n",
    "- **Open AI API Key**: Your [OpenAI API key](https://platform.openai.com/account/api-keys) is read from the `$OPENAI_API_KEY` environment variable if it's set, but you can override it in this notebook. When you click the **Update Key** button the key you entered will be obfuscated and stored in the `OPENAI_API_KEY` global variable.\n",
    "- **Model**: You can choose between the `gtp-3.5-turbo` and `gpt-4` models for this demo. The `gpt-4` model is more powerful, but it's also slower and more expensive to use.\n",
    "- **Temperature**: A model's temperature is a measure of how \"creative\" or \"unique\" it's response will be. You can set this to `0` for something pretty close to deterministic responses to simple queries.\n",
    "- **Sample Size**: We'll be gathering the top storeis from the [Hacker News API](https://github.com/HackerNews/API) and then sending the titles of a sample of those stories to the model for analysis. For quicker, cheaper results you may want to set this to a lower number. The larger your sample, the more tokens that will be consumed and the more likely you are to hit any rate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12db0c43",
   "metadata": {
    "tags": [
     "hide-input",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "# this code cell is just used to display a widget\n",
    "# for us to configure some settings that other cells\n",
    "# in this notebook rely on\n",
    "apiKeyInput.value = obfuscateKey(OPENAI_API_KEY)\n",
    "sampleSizeSlider.value = STORY_SAMPLE_SIZE\n",
    "temperatureSlider.value = TEMPERATURE\n",
    "\n",
    "def updateApiKey(event):\n",
    "  global OPENAI_API_KEY\n",
    "  OPENAI_API_KEY = apiKeyInput.value\n",
    "  apiKeyInput.value = obfuscateKey(OPENAI_API_KEY)\n",
    "\n",
    "def updateSampleSize(change):\n",
    "  global STORY_SAMPLE_SIZE\n",
    "  STORY_SAMPLE_SIZE = change['new']\n",
    "\n",
    "def updateTemperature(change):\n",
    "  global TEMPERATURE\n",
    "  TEMPERATURE = change['new']\n",
    "\n",
    "temperatureSlider.observe(updateTemperature, names='value')\n",
    "sampleSizeSlider.observe(updateSampleSize, names='value')\n",
    "apiKeyUpdateButton.on_click(updateApiKey)\n",
    "\n",
    "apiKeyConfigWidget = pywidgets.HBox([apiKeyInput, apiKeyUpdateButton])\n",
    "openAiConfigWidget = pywidgets.VBox([openAiHeader, apiKeyConfigWidget, modelDropdown, temperatureSlider])\n",
    "hackerNewsConfigWidget = pywidgets.VBox([hackerNewsHeader, sampleSizeSlider, sampleSizeWarningLabel])\n",
    "configWidget = pywidgets.VBox([openAiConfigWidget, hackerNewsConfigWidget])\n",
    "\n",
    "display(configWidget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59708fe",
   "metadata": {},
   "source": [
    "## Simple sentiment analysis with NLTK\n",
    "\n",
    "Let's take a look at a simple example of sentiment analysis with `nltk` and VADER.\n",
    "\n",
    "The `SentimentIntensityAnalyzer` returns an object with positive, negative, and neutral scores for the given text as well as a combined `compound` score computed from the other three.\n",
    "\n",
    "For this basic example, we're going to rely on the `compound` score and use a naive rating scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec9c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the VADER sentiment analyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# instantiate the sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# analyze the sentiment of a string of text\n",
    "def analyzeSentiment(text):\n",
    "  if not text:\n",
    "    return('')\n",
    "\n",
    "  # use VADER to get the +/- sentiment of the string\n",
    "  sentiment = analyzer.polarity_scores(text)\n",
    "\n",
    "  # map the sentiment to a human readable label\n",
    "  if sentiment['compound'] >= 0.75:\n",
    "    return('very positive')\n",
    "  elif sentiment['compound'] >= 0.4:\n",
    "    return('positive')\n",
    "  elif sentiment['compound'] >= 0.1:\n",
    "    return('leaning positive')\n",
    "  elif sentiment['compound'] <= -0.1 and sentiment['compound'] > -0.4:\n",
    "    return('leaning negative')\n",
    "  elif sentiment['compound'] <= -0.4 and sentiment['compound'] > -0.75:\n",
    "    return('negative')\n",
    "  elif sentiment['compound'] <= -0.75:\n",
    "    return('very negative')\n",
    "  else:\n",
    "    return('neutral')\n",
    "  \n",
    "# some simple test statements for our analyzer\n",
    "statements = [\n",
    "  'I love that movie.',\n",
    "  'I hate that movie.',\n",
    "  'I like that movie.',\n",
    "  'I dislike that movie.',\n",
    "  'I saw that movie.',\n",
    "]\n",
    "\n",
    "for statement in statements:\n",
    "  print(f\"{statement} ({analyzeSentiment(statement)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6429f9",
   "metadata": {},
   "source": [
    "We've wired the input below up to the same analyzer function from above. Type in some text and see how the analyzer responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342011fc",
   "metadata": {
    "tags": [
     "hide-input",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "# this code cell is just used to display a widget\n",
    "# that uses the analyzeSentiment function we created\n",
    "display(simpleAnalysisWidget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bbf2a5",
   "metadata": {},
   "source": [
    "## How Sentiment Analysis Works\n",
    "\n",
    "Sentiment analysis, like most text analysis involves a multistep process:\n",
    "\n",
    "1. **Stemming / Lemmatization**: reduces the words in the text to their root forms to simplify comparison between different forms of the same words\n",
    "   1. **Stemming**: removes suffixes as an attempt to reduce words to their root forms\n",
    "   2. **Lemmatization**: uses a morphological analysis of words to reduce them to their root forms\n",
    "2. **Tokenization**: breaks the text into individual units of meaning called tokens\n",
    "3. **Vectorization**: converts the tokens into a id that can be used for comparison\n",
    "4. **Comparison**: compares the tokens to a known set of tokens to determine the sentiment\n",
    "\n",
    "In this case we're taking advantage of an existing model that has been trained to analyze sentiment in text. If we wanted to build our own from scratch, it would be a more complicated process and require training data to feed into the model.\n",
    "\n",
    "With the advent of Generative Pre-Trained Transformer (GPT) models like those that power ChatGPT, and other transformer models that have exploded in popularity since, we can leverage the powerful inference and predictive capabilities of these models to perform sentiment analysis without having to train our own model, and we can even leverage some prompting techniques to quickly teach the model how to perform more unique analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c607a30",
   "metadata": {},
   "source": [
    "## Real world example\n",
    "\n",
    "So, let's see how this works with text generated by other humans without knowing that someone would be trying to analyze the sentiment of their text.\n",
    "\n",
    "For this example, we'll pull in a random sample of the [top stories](https://github.com/HackerNews/API#new-top-and-best-stories) on [Hacker News](https://news.ycombinator.com/) and analyze the sentiment of each submission's title.\n",
    "\n",
    "You can run the cell below a few times to generate different samples of the top stories until you find a collection you prefer and then rerun the cells below to use that sample for the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e3462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getSampleStories(sampleSize = STORY_SAMPLE_SIZE):\n",
    "  topStoryIdsRequest = session.get('https://hacker-news.firebaseio.com/v0/topstories.json')\n",
    "\n",
    "  if topStoryIdsRequest.status_code != 200:\n",
    "    print('There was a problem getting the top stories from Hacker News')\n",
    "    exit()\n",
    "\n",
    "  topStoryIds = topStoryIdsRequest.json()\n",
    "\n",
    "  storyIds = np.array(topStoryIds)[np.random.choice(len(topStoryIds), sampleSize, replace=False)]\n",
    "\n",
    "  return storyIds\n",
    "\n",
    "\n",
    "def getStoryDetails(storyId):\n",
    "  # we'll use the same request cache so that we don't have to request a story's details more than once\n",
    "  storyRequest = session.get(f'https://hacker-news.firebaseio.com/v0/item/{storyId}.json')\n",
    "\n",
    "  if storyRequest.status_code != 200:\n",
    "    print(f'There was a problem getting story {storyId} from Hacker News')\n",
    "    return None\n",
    "  else:\n",
    "    story = storyRequest.json()\n",
    "\n",
    "  return story\n",
    "\n",
    "\n",
    "def getStories(storyIds):\n",
    "  stories = {}\n",
    "\n",
    "  for storyId in storyIds:\n",
    "    story = getStoryDetails(storyId)\n",
    "\n",
    "    if 'title' in story:\n",
    "      stories[storyId] = {\n",
    "        \"title\": story['title'],\n",
    "        \"time\": story['time'],\n",
    "        \"sentiment\": {\n",
    "          \"vader\": '',\n",
    "          \"nrclex\": {},\n",
    "          \"openai\": {}\n",
    "        }\n",
    "      }\n",
    "  \n",
    "  return stories\n",
    "\n",
    "\n",
    "stories = getStories(getSampleStories())\n",
    "\n",
    "for storyId, story in stories.items():\n",
    "  print(story['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b0ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeStories(stories):\n",
    "  for _, story in stories.items():\n",
    "    story['sentiment']['vader'] = analyzeSentiment(story['title'])\n",
    "    print(f\"{story['title']} ({story['sentiment']['vader']})\")\n",
    "\n",
    "\n",
    "analyzeStories(stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d164653f",
   "metadata": {},
   "source": [
    "## How ChatGPT works\n",
    "\n",
    "In responding to our prompts, ChatGPT follows a similar process to the NLP workflow described above.\n",
    "\n",
    "It breaks our prompts into tokens and then predicts which tokens should most logically follow the ones that we've provided.\n",
    "\n",
    "### Tokens\n",
    "\n",
    "Tokenization breaks text down into units of meaning, and just like the stemming/lemmatization that we discussed earlier, you'll notice that words are often broken down into their roots and suffixes.\n",
    "\n",
    "You can test out the tokenizer used by ChatGPT below, there's also a great visual breakdown available at [https://gpt-tokenizer.dev/](https://gpt-tokenizer.dev/).\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924072c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def tokenize(text):\n",
    "  tokens = []\n",
    "  ids = []\n",
    "  \n",
    "  # To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "  encoding = tiktoken.encoding_for_model(modelDropdown.value)\n",
    "\n",
    "  tokenized = encoding.encode(text)\n",
    "\n",
    "  for tokenId in tokenized:\n",
    "    ids.append(tokenId)\n",
    "    tokens.append(encoding.decode_single_token_bytes(tokenId).decode('utf-8'))\n",
    "\n",
    "  return (tokens, ids)\n",
    "\n",
    "statements = [\n",
    "  'I love that movie.',\n",
    "  'I hate that movie.',\n",
    "  'I like that movie.',\n",
    "  'I dislike that movie.',\n",
    "  'I saw that movie.',\n",
    "]\n",
    "\n",
    "for statement in statements:\n",
    "  (statementTokens, statementIds) = tokenize(statement)\n",
    "  print(f\"{statementTokens} ({len(statementTokens)} tokens)\")\n",
    "  print(f\"{statementIds}\")\n",
    "  print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40566893",
   "metadata": {},
   "source": [
    "We've wired the input below up to the same tokenizer function from above. Type in some text and see how the tokenizer responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45afdc5",
   "metadata": {
    "tags": [
     "hide-input",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "# this code cell is just used to display a widget\n",
    "configureModel(modelDropdown.value)\n",
    "\n",
    "display(tokenAnalysisWidget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc95505",
   "metadata": {},
   "source": [
    "## Prompt engineering\n",
    "\n",
    "[Prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering), prompt design, or simply \"prompting\" is the process of creating and testing instructions for the model (called \"prompts\") to find the right instructions that return your desired results as often as possible.\n",
    "\n",
    "Generally, each message you send and each response that you receive become part of the overall prompt for the next message.\n",
    "\n",
    "You can think of the overall conversation as a document of text - it can help to imagine it as a screenplay.\n",
    "\n",
    "There are various types of messages that make up this screenplay:\n",
    "\n",
    "- **System**: system messages are sort of like stage directions, they describe the overall parameters that the model should follow and provide any other context that the model might need to know about as the conversation continues\n",
    "- **User**: user messages are the individual prompts that the user sends to the model\n",
    "- **Assistant**: assistant messages are the responses the model generates to the user's prompts\n",
    "\n",
    "The whole thing looks a bit like this:\n",
    "\n",
    "```\n",
    "[System]\n",
    "Assistant is a large language model trained by OpenAI.\n",
    "Knowledge cutoff: 2021-09\n",
    "Current date: 2023-08-17\n",
    "Browsing: disabled\n",
    "\n",
    "[User]\n",
    "Who won the Super Bowl in 2022?\n",
    "\n",
    "[Assistant]\n",
    "I'm sorry, but I don't have access to real-time information as my knowledge was last updated in September 2021. To find out the winner of the Super Bowl in 2022, I recommend checking a reliable sports news website, a sports app, or conducting a quick internet search for the most up-to-date information.\n",
    "```\n",
    "\n",
    "**Note**: There are also messages with the type `function` that indicate that the Assistant would like to execute the function with the given name and pass it the given parameters, but for this demo, we'll be ignoring those.\n",
    "\n",
    "### Basic example\n",
    "\n",
    "Here's an example of a basic prompt for seniment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c237a87",
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1694090931628,
     "user": {
      "displayName": "Matthias Kraft",
      "userId": "07587708083169164935"
     },
     "user_tz": -120
    },
    "id": "3c237a87"
   },
   "outputs": [],
   "source": [
    "BASIC_SYSTEM_PROMPT = \"\"\"\n",
    "You are VibeCheck, an advanced AI system for detecting the sentiment conveyed in user-generated text.\n",
    "\n",
    "The user will provide you with a prompt, and you will respond with the sentiment of that prompt.\n",
    "\n",
    "Do not include any punctuation and only use lower case letters.\n",
    "\"\"\"\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def basicChatGptSentiment(prompt, model=modelDropdown.value):\n",
    "    messages = [{ \"role\": \"system\", \"content\": BASIC_SYSTEM_PROMPT }]\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556a5d9b",
   "metadata": {},
   "source": [
    "Let's apply this to our Hacker News example from earlier and compare the sentiment detected by ChatGPT to the sentiment we received from the VADER analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_API_KEY:\n",
    "  for storyId, story in stories.items():\n",
    "    sentiment = basicChatGptSentiment(story['title'])\n",
    "    \n",
    "    if modelDropdown.value not in story['sentiment']['openai']:\n",
    "      story['sentiment']['openai'][modelDropdown.value] = {}\n",
    "\n",
    "    story['sentiment']['openai'][modelDropdown.value]['basic'] = sentiment\n",
    "\n",
    "    print(f\"{story['title']}\\nVADER: {story['sentiment']['vader']}\\n{modelDropdown.value}: {sentiment}\\n---\")\n",
    "else:\n",
    "  print('Please enter your OpenAI API key above and rerun this cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f5236",
   "metadata": {},
   "source": [
    "### Going further\n",
    "\n",
    "What if we wanted to dig a bit deeper and consider the emotions that might be associated with some text rather than just a simple positive to negative spectrum?\n",
    "\n",
    "There are tools like [NRCLex](https://pypi.org/project/nrclex/) that can help us with this, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bdb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrclex import NRCLex\n",
    "\n",
    "def getNRCEmotion(text):\n",
    "  emotion = NRCLex(text)\n",
    "\n",
    "  return emotion.top_emotions\n",
    "\n",
    "for storyId, story in stories.items():\n",
    "  emotions = []\n",
    "\n",
    "  emotionAnalysis = getNRCEmotion(story['title'])\n",
    "\n",
    "  for (emotion, value) in emotionAnalysis:\n",
    "    if value > 0.00:\n",
    "      emotions.append(emotion)\n",
    "\n",
    "  story['sentiment']['nrclex'] = \", \".join(emotions)\n",
    "\n",
    "  print(f\"{story['title']} {('(' + ', '.join(emotions) + ')') if len(emotions) else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740845f9",
   "metadata": {},
   "source": [
    "But, with how short some of our titles are, it doesn't always seem to get good results and it seems like sometimes it disagrees with the VADER sentiment analysis, too.\n",
    "\n",
    "We could try to train a model to do this for us, but that would be a lot of work and we'd need a lot of data to train it on.\n",
    "\n",
    "Luckily, we can pretty easily adapt our initial prompt to get ChatGPT to do this for us, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e479fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADVANCED_SYSTEM_PROMPT = \"\"\"\n",
    "You are VibeCheck, an advanced AI system for detecting the sentiment conveyed in user-generated text.\n",
    "\n",
    "The user will provide you with a prompt, and you will analyze it following these steps:\n",
    "\n",
    "1. Analyze the prompt for relevant emotion, tone, affinity, sarcasm, irony, etc.\n",
    "2. Analyze the likely emotional state of the author based on those findings\n",
    "3. Summarize the emotional state and sentiment of the prompt based on your findings using 5 or less names for emotions using lowercase letters and separating each emotional state with a comma\n",
    "\n",
    "Only return the output from the final step to the user.\n",
    "\"\"\"\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def advancedChatGptSentiment(prompt, model=modelDropdown.value):\n",
    "    messages = [{ \"role\": \"system\", \"content\": ADVANCED_SYSTEM_PROMPT }]\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2aee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_API_KEY:\n",
    "  for storyId, story in stories.items():\n",
    "    sentiment = advancedChatGptSentiment(story['title'])\n",
    "\n",
    "    if modelDropdown.value not in story['sentiment']['openai']:\n",
    "      story['sentiment']['openai'][modelDropdown.value] = {}\n",
    "\n",
    "    story['sentiment']['openai'][modelDropdown.value]['advanced'] = sentiment\n",
    "\n",
    "    print(f\"{story['title']} ({sentiment})\")\n",
    "else:\n",
    "  print('Please enter your OpenAI API key above and rerun this cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59daa29b",
   "metadata": {
    "tags": [
     "hide-input",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "# this code cell is just used to display a widget\n",
    "# that uses the analyzeSentiment function we created\n",
    "# as well as the advancedChatGptSentiment function\n",
    "configureOpenAi(OPENAI_API_KEY, modelDropdown.value, TEMPERATURE)\n",
    "\n",
    "display(advancedAnalysisWidget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b05dc",
   "metadata": {},
   "source": [
    "## Beyond sentiment\n",
    "\n",
    "What if we were looking to do something a little more complicated than just sentiment analysis?\n",
    "\n",
    "What if we wanted to describe the sentiment of some text via an emoji?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0058f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOJI_SYSTEM_PROMPT = \"\"\"\n",
    "You are VibeCheck, an advanced AI system for detecting the sentiment conveyed in user-generated text.\n",
    "\n",
    "The user will provide you with a prompt, and you will analyze it following these steps:\n",
    "\n",
    "1. Analyze the prompt for relevant emotion, tone, affinity, sarcasm, irony, etc.\n",
    "2. Analyze the likely emotional state of the author based on those findings\n",
    "3. Summarize the emotional state and sentiment of the prompt based on your findings into 5 or fewer names for emotional states\n",
    "4. Convert each of the emotional states you identified into a representative emoji\n",
    "\n",
    "Only return the output from the final step to the user.\n",
    "\"\"\"\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def emojiChatGptSentiment(prompt, model=modelDropdown.value):\n",
    "    messages = [{ \"role\": \"system\", \"content\": EMOJI_SYSTEM_PROMPT }]\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f3196",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_API_KEY:\n",
    "  for storyId, story in stories.items():\n",
    "    sentiment = emojiChatGptSentiment(story['title'])\n",
    "\n",
    "    if modelDropdown.value not in story['sentiment']['openai']:\n",
    "      story['sentiment']['openai'][modelDropdown.value] = {}\n",
    "\n",
    "    story['sentiment']['openai'][modelDropdown.value]['emoji'] = sentiment\n",
    "\n",
    "    print(f\"{story['title']}({sentiment})\")\n",
    "else:\n",
    "  print('Please enter your OpenAI API key above and rerun this cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda6a510",
   "metadata": {},
   "source": [
    "## Prompting strategies\n",
    "\n",
    "In the previous examples we've been using \"zero shot\" prompting, which means we're asking the model to repsond without giving it an example of what kind of response we'd like for it to have.\n",
    "\n",
    "There are other prompting strategies we can employ, though:\n",
    "\n",
    "- **One shot**: gives the model a single example of how we'd like it to respond to guide it's output\n",
    "- **Few shot**: gives the model a few examples of how we'd like it to respond to different prompts to help guide it's output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc5cfb3",
   "metadata": {},
   "source": [
    "### One shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbed from https://news.ycombinator.com/ at 2023-09-20 13:00 EDT\n",
    "# Reference: https://news.ycombinator.com/item?id=37598299\n",
    "ONE_SHOT_USER_EXAMPLE = \"Cisco pulled out of the SentinelOne acquisition after due dilligence\"\n",
    "\n",
    "ONE_SHOT_BOT_EXAMPLE = \"ü§®\"\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def oneShotChatGptSentiment(prompt, model=modelDropdown.value):\n",
    "    messages = [\n",
    "        { \"role\": \"system\", \"content\": EMOJI_SYSTEM_PROMPT },\n",
    "        {\"role\": \"user\", \"content\": ONE_SHOT_USER_EXAMPLE },\n",
    "        {\"role\": \"assistant\", \"content\": ONE_SHOT_BOT_EXAMPLE }\n",
    "    ]\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb4d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_API_KEY:\n",
    "  for storyId, story in stories.items():\n",
    "    sentiment = oneShotChatGptSentiment(story['title'])\n",
    "\n",
    "    if modelDropdown.value not in story['sentiment']['openai']:\n",
    "      story['sentiment']['openai'][modelDropdown.value] = {}\n",
    "\n",
    "    story['sentiment']['openai'][modelDropdown.value]['oneshot'] = sentiment\n",
    "\n",
    "    print(f\"{story['title']}({sentiment})\")\n",
    "else:\n",
    "  print('Please enter your OpenAI API key above and rerun this cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83570356",
   "metadata": {},
   "source": [
    "### Few shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbed from https://news.ycombinator.com/ at 2023-09-20 13:10 EDT\n",
    "FEW_SHOT_USER_EXAMPLES = [\n",
    "  # Reference: https://news.ycombinator.com/item?id=37598299\n",
    "  \"Cisco pulled out of the SentinelOne acquisition after due dilligence\",\n",
    "  # Reference: https://news.ycombinator.com/item?id=37595898\n",
    "  \"Atlassian cripples Jira automation for all but enterprise customers\",\n",
    "  # Reference: https://news.ycombinator.com/item?id=37586264\n",
    "  \"Toyota Research claims breakthrough in teaching robots new behaviors\"\n",
    "]\n",
    "\n",
    "FEW_SHOT_BOT_EXAMPLES = [\n",
    "  \"ü§®\",\n",
    "  \"üòñ\",\n",
    "  \"üëè\",\n",
    "]\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def fewShotChatGptSentiment(prompt, model=modelDropdown.value):\n",
    "    messages = [{ \"role\": \"system\", \"content\": EMOJI_SYSTEM_PROMPT }]\n",
    "\n",
    "    for i, userExample in enumerate(FEW_SHOT_USER_EXAMPLES):\n",
    "        messages.append({\"role\": \"user\", \"content\": userExample })\n",
    "        messages.append({\"role\": \"assistant\", \"content\": FEW_SHOT_BOT_EXAMPLES[i] })\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67080489",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_API_KEY:\n",
    "  for storyId, story in stories.items():\n",
    "    sentiment = fewShotChatGptSentiment(story['title'])\n",
    "\n",
    "    if modelDropdown.value not in story['sentiment']['openai']:\n",
    "      story['sentiment']['openai'][modelDropdown.value] = {}\n",
    "\n",
    "    story['sentiment']['openai'][modelDropdown.value]['fewshot'] = sentiment\n",
    "\n",
    "    print(f\"{story['title']} ({sentiment})\")\n",
    "else:\n",
    "  print('Please enter your OpenAI API key above and rerun this cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e4186e",
   "metadata": {},
   "source": [
    "## Comparing methods\n",
    "\n",
    "Let's compare the sentiment analysis results of the different approaches we've taken in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc761bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b4ceb9",
   "metadata": {},
   "source": [
    "## Comparing approaches\n",
    "\n",
    "We've looked at various approaches to analyzing sentiment and explored some interesting and novel ways that we can work with AI models like ChatGPT to perform tasks that used to require large investments of time to gather and label data and then train a model.\n",
    "\n",
    "Let's compare the results of each analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e406deb6",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentimentData = {\n",
    "  \"Story\": [],\n",
    "  \"VADER\": [],\n",
    "  \"NRC\": [],\n",
    "  \"ChatGPT (Sentiment)\": [],\n",
    "  \"ChatGPT (Emotion)\": [],\n",
    "  \"Zero Shot\": [],\n",
    "  \"One Shot\": [],\n",
    "  \"Few Shot\": [],\n",
    "}\n",
    "\n",
    "for storyId, story in stories.items():\n",
    "  if 'title' in story:\n",
    "    sentimentData[\"Story\"].append(story['title'])\n",
    "  \n",
    "  if 'vader' in story['sentiment']:\n",
    "    sentimentData[\"VADER\"].append(story['sentiment']['vader'])\n",
    "  \n",
    "  if 'nrclex' in story['sentiment']:\n",
    "    sentimentData[\"NRC\"].append(story['sentiment']['nrclex'])\n",
    "\n",
    "  if 'openai' in story['sentiment'] and modelDropdown.value in story['sentiment']['openai']:\n",
    "    if 'basic' in story['sentiment']['openai'][modelDropdown.value]:\n",
    "      sentimentData[\"ChatGPT (Sentiment)\"].append(story['sentiment']['openai'][modelDropdown.value]['basic'])\n",
    "\n",
    "    if 'advanced' in story['sentiment']['openai'][modelDropdown.value]:\n",
    "      sentimentData[\"ChatGPT (Emotion)\"].append(story['sentiment']['openai'][modelDropdown.value]['advanced'])\n",
    "\n",
    "    if 'emoji' in story['sentiment']['openai'][modelDropdown.value]:\n",
    "      sentimentData[\"Zero Shot\"].append(story['sentiment']['openai'][modelDropdown.value]['emoji'])\n",
    "\n",
    "    if 'oneshot' in story['sentiment']['openai'][modelDropdown.value]:\n",
    "      sentimentData[\"One Shot\"].append(story['sentiment']['openai'][modelDropdown.value]['oneshot'])\n",
    "\n",
    "    if 'fewshot' in story['sentiment']['openai'][modelDropdown.value]:\n",
    "      sentimentData[\"Few Shot\"].append(story['sentiment']['openai'][modelDropdown.value]['fewshot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2ddf1",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# this cell is only used to display a dataframe of our sentiment analysis results\n",
    "sentimentDataFrame = pd.DataFrame(data=sentimentData, columns=[\"Story\", \"VADER\", \"ChatGPT (Sentiment)\"], )\n",
    "\n",
    "display(sentimentDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb674f0f",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# this code cell is only used to display a dataframe with our emotional analysis results\n",
    "emotionDataFrame = pd.DataFrame(data=sentimentData, columns=[\"Story\", \"NRC\", \"ChatGPT (Emotion)\"])\n",
    "\n",
    "# often NRCLex will not have data and instead of displaying NaN we'll leave it blank\n",
    "emotionDataFrame = emotionDataFrame.fillna('')\n",
    "\n",
    "display(emotionDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d8160",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# this cell is just used to display a dataframe with our emoji results\n",
    "emojiDataFrame = pd.DataFrame(data=sentimentData, columns=[\"Story\", \"Zero Shot\", \"One Shot\", \"Few Shot\"])\n",
    "\n",
    "display(emojiDataFrame)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
