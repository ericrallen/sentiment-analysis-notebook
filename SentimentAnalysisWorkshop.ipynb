{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b5de47",
   "metadata": {
    "tags": [
     "hide-cell",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%pip install openai nltk ipywidgets numpy requests-cache backoff tiktoken nrclex pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f29fd",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with ChatGPT\n",
    "\n",
    "While sentiment analysis is sort of like the [\"Hello, world!\"](https://en.wikipedia.org/wiki/%22Hello,_World!%22_program#Variations) of [Natural Language Processing](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP), luckily for us it's a bit more fun than just echoing out a string.\n",
    "\n",
    "This notebook will introduce you to sentiment analysis using traditional NLP tools and then explore analyzing sentiment with [ChatGPT](https://openai.com/blog/chatgpt).\n",
    "\n",
    "**Note**: For a better learning experience, this notebook contains some code cells that are only used to render widgets for you to interact with and some others that only generate data structures or variables that later cells will reference.\n",
    "\n",
    "## What is sentiment analysis?\n",
    "\n",
    "[Sentiment Analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) is a way of analyzing some text to determine if it's positive, negative, or neutral.\n",
    "\n",
    "This is the kind of thing that's pretty easy for a human who understands the language the text is written in to do, but it can be hard for a computer to really understand the underlying meaning behind the language.\n",
    "\n",
    "### Examples\n",
    "\n",
    "- \"I saw that movie.\" (neutral)\n",
    "- \"I love that movie.\" (positive)\n",
    "- \"I hate that movie.\" (negative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f45b4b3",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "\n",
    "First, we'll import the relevant tools we'll be using in the notebook and configure some global variables.\n",
    "\n",
    "- `nltk`: Python's [Natural Language Toolkit](https://www.nltk.org/), which we'll use to explore some more traditional sentiment analysis techniques\n",
    "- `openai`: Python library for interacting with the [OpenAI API](https://platform.openai.com/docs/api-reference/introduction)\n",
    "\n",
    "**Note**: In a later cell, we'll also make use of [`nrclex`](https://github.com/metalcorebear/NRCLex) to investigate some more advanced NLP, but because it's only used in one cell, we're importing it there for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e3c67",
   "metadata": {
    "tags": [
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nltk\n",
    "import openai\n",
    "\n",
    "# download nltk data\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# globals\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TEMPERATURE = 0.37\n",
    "STORY_SAMPLE_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db52a5a8",
   "metadata": {},
   "source": [
    "You'll be able to configure these global variables using an embedded widget form below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d1782d",
   "metadata": {
    "tags": [
     "hide-cell",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "# this cell focuses on some implemetation details specific to\n",
    "# this notebook that aren't actually important to understand\n",
    "# you can just ignore/collapse it if you would prefer\n",
    "import ipywidgets as pywidgets\n",
    "import requests as request\n",
    "import requests_cache\n",
    "import backoff\n",
    "\n",
    "# configuration widgets\n",
    "from widgets.config import (\n",
    "    modelDropdown,\n",
    "    apiKeyInput,\n",
    "    apiKeyUpdateButton,\n",
    "    temperatureSlider,\n",
    "    sampleSizeSlider,\n",
    "    sampleSizeWarningLabel,\n",
    "    openAiHeader,\n",
    "    hackerNewsHeader,\n",
    ")\n",
    "\n",
    "# project-specific widgets\n",
    "from widgets.simple import simpleAnalysisWidget\n",
    "from widgets.advanced import advancedAnalysisWidget, configureOpenAi\n",
    "from widgets.tokens import tokenAnalysisWidget, configureModel\n",
    "\n",
    "# project-specific utilities\n",
    "from utils.obfuscate import obfuscateKey\n",
    "from utils.array import checkArrayLengths\n",
    "\n",
    "# we don't want to display too many entries in our DataFrames\n",
    "# if the sample size is too large\n",
    "DATAFRAME_LIMIT = 20\n",
    "\n",
    "# we'll use this session to cache our hacker news api requests\n",
    "REQUEST_CACHE_EXPIRATION_SECONDS = 60 * 15\n",
    "session = requests_cache.CachedSession(\n",
    "    \"hackernews_cache\", expire_after=REQUEST_CACHE_EXPIRATION_SECONDS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9faba98",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "You can make changes to the configuration form below at any time and rerun cells that make requests to the OpenAI API or Hacker News API to see how the results change.\n",
    "\n",
    "You can configure the following values:\n",
    "\n",
    "- **Open AI API Key**: Your [OpenAI API key](https://platform.openai.com/account/api-keys) is read from the `$OPENAI_API_KEY` environment variable if it's set, but you can override it in this notebook; when you click the **Update Key** button the key you entered will be obfuscated and stored in the `OPENAI_API_KEY` global variable\n",
    "- **Model**: The [OpenAI model](https://platform.openai.com/docs/models) that the demo should use; you can choose between the `gtp-3.5-turbo` and `gpt-4` models for this demo\n",
    "- **Temperature**: A model's [temperature](https://platform.openai.com/docs/guides/gpt/how-should-i-set-the-temperature-parameter) is a measure of how \"creative\" it's response will be; you can set this to `0` for something pretty close to deterministic responses to simple queries\n",
    "- **Sample Size**: We'll be gathering the top stories from the [Hacker News API](https://github.com/HackerNews/API) and then analyzing the sentiment of a sample of those stories' titles; this controls how large that sample is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12db0c43",
   "metadata": {
    "tags": [
     "hide-input",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "# this code cell is just used to display a widget for us to\n",
    "# configure some settings that other cells in this notebook rely on\n",
    "# you can just ignore/collapse it if you would prefer\n",
    "apiKeyInput.value = obfuscateKey(OPENAI_API_KEY)\n",
    "sampleSizeSlider.value = STORY_SAMPLE_SIZE\n",
    "temperatureSlider.value = TEMPERATURE\n",
    "\n",
    "\n",
    "def updateApiKey(event):\n",
    "    global OPENAI_API_KEY\n",
    "    OPENAI_API_KEY = apiKeyInput.value\n",
    "    apiKeyInput.value = obfuscateKey(OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "def updateSampleSize(change):\n",
    "    global STORY_SAMPLE_SIZE\n",
    "    STORY_SAMPLE_SIZE = change[\"new\"]\n",
    "\n",
    "\n",
    "def updateTemperature(change):\n",
    "    global TEMPERATURE\n",
    "    TEMPERATURE = change[\"new\"]\n",
    "\n",
    "\n",
    "temperatureSlider.observe(updateTemperature, names=\"value\")\n",
    "sampleSizeSlider.observe(updateSampleSize, names=\"value\")\n",
    "apiKeyUpdateButton.on_click(updateApiKey)\n",
    "\n",
    "apiKeyConfigWidget = pywidgets.HBox([apiKeyInput, apiKeyUpdateButton])\n",
    "openAiConfigWidget = pywidgets.VBox(\n",
    "    [openAiHeader, apiKeyConfigWidget, modelDropdown, temperatureSlider]\n",
    ")\n",
    "hackerNewsConfigWidget = pywidgets.VBox(\n",
    "    [hackerNewsHeader, sampleSizeSlider, sampleSizeWarningLabel]\n",
    ")\n",
    "configWidget = pywidgets.VBox([openAiConfigWidget, hackerNewsConfigWidget])\n",
    "\n",
    "display(configWidget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59708fe",
   "metadata": {},
   "source": [
    "## Simple sentiment analysis with NLTK\n",
    "\n",
    "Let's take a look at a simple example of sentiment analysis with `nltk` using the **V**alence **A**ware **D**ictionary and s**E**ntiment **R**easoner ([VADER](https://vadersentiment.readthedocs.io/en/latest/pages/introduction.html)) module.\n",
    "\n",
    "VADER's `SentimentIntensityAnalyzer` returns an object with positive, negative, and neutral scores for the given text as well as a combined `compound` score computed from the other three.\n",
    "\n",
    "For this basic example, we're going to rely on the `compound` score and create a naive rating scale that converts that score into a string ranging from `very positive` to `very negative`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec9c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def convertSentimentToLabel(sentiment):\n",
    "    sentimentScore = sentiment[\"compound\"]\n",
    "\n",
    "    if sentimentScore >= 0.75:\n",
    "        return \"very positive\"\n",
    "    elif sentimentScore >= 0.4:\n",
    "        return \"positive\"\n",
    "    elif sentimentScore >= 0.1:\n",
    "        return \"leaning positive\"\n",
    "    elif sentimentScore <= -0.1 and sentimentScore > -0.4:\n",
    "        return \"leaning negative\"\n",
    "    elif sentimentScore <= -0.4 and sentimentScore > -0.75:\n",
    "        return \"negative\"\n",
    "    elif sentimentScore <= -0.75:\n",
    "        return \"very negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "\n",
    "def analyzeSentiment(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    return analyzer.polarity_scores(text)\n",
    "\n",
    "\n",
    "# some simple test statements for our analyzer\n",
    "statements = [\n",
    "    \"I love that movie.\",\n",
    "    \"I hate that movie.\",\n",
    "    \"I like that movie.\",\n",
    "    \"I dislike that movie.\",\n",
    "    \"I saw that movie.\",\n",
    "]\n",
    "\n",
    "for statement in statements:\n",
    "    print(f\"{statement} ({convertSentimentToLabel(analyzeSentiment(statement))})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6429f9",
   "metadata": {},
   "source": [
    "We've wired the input below up to the same analyzer function from above. Type in some text and see how the analyzer responds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342011fc",
   "metadata": {
    "tags": [
     "hide-input",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "# this code cell is just used to display a widget\n",
    "# that uses the analyzeSentiment function we created\n",
    "# you can just ignore/collapse it if you would prefer\n",
    "display(simpleAnalysisWidget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bbf2a5",
   "metadata": {},
   "source": [
    "## How it works\n",
    "\n",
    "Sentiment analysis, like most text analysis involves a multistep process:\n",
    "\n",
    "1. **Stemming / Lemmatization**: reduces the words in the text to their root forms to simplify comparison between different forms of the same words\n",
    "   1. **Stemming**: removes suffixes as an attempt to reduce words to their root forms\n",
    "   2. **Lemmatization**: uses a morphological analysis of words to reduce them to their root forms\n",
    "2. **Tokenization**: breaks the text into individual units of meaning called tokens\n",
    "3. **Vectorization**: converts the tokens into a id that can be used for comparison\n",
    "4. **Comparison**: compares the tokens to a known set of tokens to determine the sentiment\n",
    "\n",
    "**Note**: This is a simplification of the process to distill it into an easy to digest format, but it is not a full picture and doesn't include the data gathering, cleaning, and labeling or actual training process.\n",
    "\n",
    "### Learn more\n",
    "\n",
    "- [Tokenization, Stemming, and Lemmatization in Python](https://thepythoncode.com/article/tokenization-stemming-and-lemmatization-in-python)\n",
    "- [Python for NLP: Tokenization, Stemming, and Lemmatization with SpaCy Library](https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/)\n",
    "- [What is Tokenization in Natural Language Processing (NLP)?](https://www.machinelearningplus.com/nlp/what-is-tokenization-in-natural-language-processing/)\n",
    "- [Understanding NLP Word Embeddings ‚Äî Text Vectorization](https://towardsdatascience.com/understanding-nlp-word-embeddings-text-vectorization-1a23744f7223)\n",
    "\n",
    "### Language models\n",
    "\n",
    "In this case we're taking advantage of an existing [language model](https://en.wikipedia.org/wiki/Language_model), VADER, that has been trained to analyze sentiment in text, but if we wanted to train our own model, it would be a much more involved process.\n",
    "\n",
    "With the advent of [Large Language Models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs), like the [Generative Pre-Trained Transformer](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer) (GPT) models that power ChatGPT [large language models have exploded in popularity](https://informationisbeautiful.net/visualizations/the-rise-of-generative-ai-large-language-models-llms-like-chatgpt/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da92b914",
   "metadata": {},
   "source": [
    "### LLM family tree\n",
    "\n",
    "<div style=\"display: flex; alight-items: center; justify-content: center;\"><a href=\"https://github.com/Mooler0410/LLMsPracticalGuide\" target=\"_blank\"><img alt=\"LLM Evolutionary Tree\" src=\"./assets/llm-family-tree.gif\" /><a/></div>\n",
    "\n",
    "This visualiztion from [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond](https://arxiv.org/abs/2304.13712) provides a great overview of how language models have evolved over time and gives you a sense of just how much things have been developing in the last 12 months.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabe668a",
   "metadata": {},
   "source": [
    "### The power of LLMs\n",
    "\n",
    "We can leverage the inference and predictive capabilities of these models to perform tasks like sentiment analysis with greater accuracy without having to train our own models.\n",
    "\n",
    "We can even leverage some prompting techniques - which we'll explore in later cells - to quickly teach the model how to perform more unique analyses and refine our results.\n",
    "\n",
    "In the past, these would have been a significant undertaking, but now we can acheive similar results with some simple prompting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c607a30",
   "metadata": {},
   "source": [
    "## Real world data\n",
    "\n",
    "Let's take a look at how this works with text generated by other humans (_probably_) without expecting someone would be trying to analyze the sentiment of their text.\n",
    "\n",
    "For this example, we'll pull in a random sample of the [top stories](https://github.com/HackerNews/API#new-top-and-best-stories) on [Hacker News](https://news.ycombinator.com/) and analyze the sentiment of each submission's title.\n",
    "\n",
    "You can run the cell below a few times to generate different samples of the top stories until you find a collection you prefer and then rerun the cells after it to use that sample for the rest of the notebook.\n",
    "\n",
    "**Note**: You can use the configuration widget above to adjust your sample size to find the collection of data that feels right to you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e3462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sampleStories(sampleSize=STORY_SAMPLE_SIZE):\n",
    "    topStoryIdsRequest = session.get(\n",
    "        \"https://hacker-news.firebaseio.com/v0/topstories.json\"\n",
    "    )\n",
    "\n",
    "    if topStoryIdsRequest.status_code != 200:\n",
    "        print(\"There was a problem getting the top stories from Hacker News\")\n",
    "        exit()\n",
    "\n",
    "    topStoryIds = topStoryIdsRequest.json()\n",
    "\n",
    "    storyIds = np.array(topStoryIds)[\n",
    "        np.random.choice(len(topStoryIds), sampleSize, replace=False)\n",
    "    ]\n",
    "\n",
    "    return storyIds\n",
    "\n",
    "\n",
    "def getStoryDetails(storyId):\n",
    "    # we'll use the same request cache so that we don't have to request a story's details more than once\n",
    "    storyRequest = session.get(\n",
    "        f\"https://hacker-news.firebaseio.com/v0/item/{storyId}.json\"\n",
    "    )\n",
    "\n",
    "    if storyRequest.status_code != 200:\n",
    "        print(f\"There was a problem getting story {storyId} from Hacker News\")\n",
    "        return None\n",
    "    else:\n",
    "        story = storyRequest.json()\n",
    "\n",
    "    return story\n",
    "\n",
    "\n",
    "def getStories(storyIds):\n",
    "    stories = {}\n",
    "\n",
    "    for storyId in storyIds:\n",
    "        story = getStoryDetails(storyId)\n",
    "\n",
    "        if \"title\" in story:\n",
    "            stories[storyId] = {\n",
    "                \"title\": story[\"title\"],\n",
    "                \"time\": story[\"time\"],\n",
    "                \"sentiment\": {\"vader\": \"\", \"nrclex\": {}, \"openai\": {}},\n",
    "            }\n",
    "\n",
    "    return stories\n",
    "\n",
    "\n",
    "stories = getStories(sampleStories())\n",
    "\n",
    "for storyId, story in stories.items():\n",
    "    print(story[\"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ff247f",
   "metadata": {},
   "source": [
    "Let's see what VADER thinks about the sentiment of these titles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b0ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeStories(stories):\n",
    "    for _, story in stories.items():\n",
    "        story[\"sentiment\"][\"vader\"] = convertSentimentToLabel(\n",
    "            analyzeSentiment(story[\"title\"])\n",
    "        )\n",
    "        print(f\"{story['title']} ({story['sentiment']['vader']})\")\n",
    "\n",
    "\n",
    "analyzeStories(stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a216d632",
   "metadata": {},
   "source": [
    "While this is easy enough to implement and might give us a general idea of the sentiment, what if we want to push things a little further?\n",
    "\n",
    "What if we have more complex text to analyze or have content that VADER's training doesn't handle well?\n",
    "\n",
    "We could train our own model, but that's a lot of work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d164653f",
   "metadata": {},
   "source": [
    "## ChatGPT\n",
    "\n",
    "ChatGPT is an LLM that makes use of GPT architecture combined with [Instruction Tuning](https://openai.com/research/instruction-following) to follow instructions and generate text based on the prompts that we provide.\n",
    "\n",
    "It's training data includes a whole bunch of stuff that we've all posted on the Internet over the years, as well as lots of other content.\n",
    "\n",
    "This vast trove of training data, combined with the flexibility provided by it's architecture and tuning, gives ChatGPT an impressive ability to respond to our requests for many tasks without needing to be retrained or [fine-tuned](https://www.lakera.ai/insights/llm-fine-tuning-guide) for a specific task.\n",
    "\n",
    "### How ChatGPT works\n",
    "\n",
    "In responding to our prompts, ChatGPT follows a similar process to the NLP workflow described above.\n",
    "\n",
    "It breaks our prompts into [tokens](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/tokens), predicts which tokens should logically follow the ones that we've provided, and returns that text.\n",
    "\n",
    "ChatGPT's tuning based on Reinforcement Learning from Human Feedback ([RLHF](https://www.assemblyai.com/blog/how-rlhf-preference-model-tuning-works-and-how-things-may-go-wrong/)) is what lead it to be so popular, and is also part of what makes it so powerful.\n",
    "\n",
    "#### Learn more\n",
    "\n",
    "- [How ChatGPT Actually Works](https://www.assemblyai.com/blog/how-chatgpt-actually-works/)\n",
    "- [How ChatGPT Works: The Models Behind The Bot](https://towardsdatascience.com/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286)\n",
    "- [The inside story of how ChatGPT was built from the people who made it](https://www.technologyreview.com/2023/03/03/1069311/inside-story-oral-history-how-chatgpt-built-openai/)\n",
    "\n",
    "### Tokens\n",
    "\n",
    "Tokenization breaks text down into units of meaning, and just like the stemming/lemmatization that we discussed earlier, you'll notice that words are often broken down into their roots and suffixes when tokenized by ChatGPT's Byte Pair Encoding ([BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding)) tokenization algorithm, [tiktoken](https://github.com/openai/tiktoken).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924072c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = []\n",
    "    ids = []\n",
    "\n",
    "    # To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "    encoding = tiktoken.encoding_for_model(modelDropdown.value)\n",
    "\n",
    "    tokenized = encoding.encode(text)\n",
    "\n",
    "    for tokenId in tokenized:\n",
    "        ids.append(tokenId)\n",
    "        tokens.append(encoding.decode_single_token_bytes(tokenId).decode(\"utf-8\"))\n",
    "\n",
    "    return (tokens, ids)\n",
    "\n",
    "\n",
    "statements = [\n",
    "    \"I love that movie.\",\n",
    "    \"I hate that movie.\",\n",
    "    \"I like that movie.\",\n",
    "    \"I dislike that movie.\",\n",
    "    \"I saw that movie.\",\n",
    "]\n",
    "\n",
    "for statement in statements:\n",
    "    (statementTokens, statementIds) = tokenize(statement)\n",
    "    print(f\"{statementTokens} ({len(statementTokens)} tokens)\")\n",
    "    print(f\"{statementIds}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40566893",
   "metadata": {},
   "source": [
    "We've wired the input below up to the same tokenizer function above. Type in some text and see how the tokenizer responds.\n",
    "\n",
    "There's also a great visualizer available at [https://gpt-tokenizer.dev/](https://gpt-tokenizer.dev/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45afdc5",
   "metadata": {
    "tags": [
     "hide-input",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "# this code cell is just used to display a widget\n",
    "# that uses the tokenize function we created\n",
    "# you can just ignore/collapse it if you would prefer\n",
    "configureModel(modelDropdown.value)\n",
    "\n",
    "display(tokenAnalysisWidget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc95505",
   "metadata": {},
   "source": [
    "## Prompt engineering\n",
    "\n",
    "[Prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering) (or \"prompting\" if you are into the whole brevity thing) is the process of creating and testing instructions for the model (called \"prompts\") to find the most concise set of instructions that will guide the model towards returning your desired results as often as possible while minimizing undesired output like [hallucinations](<https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)>) and [apologies](https://news.ycombinator.com/item?id=36949931).\n",
    "\n",
    "In general, each message you send and each response that you receive become part of the overall prompt for the next message, but there are strategies for managing a conversation's memory in order to selectively exclude messages that might lead to the model getting off track if repeated often enough.\n",
    "\n",
    "You can think of the overall conversation as a document of text - it can help to imagine it as something like a [screenplay](https://en.wikipedia.org/wiki/Screenplay).\n",
    "\n",
    "There are various types of messages that make up this screenplay:\n",
    "\n",
    "- **System**: system messages are sort of like stage directions, they describe the overall parameters that the model should follow and provide any other context that the model might need to know about as the conversation continues\n",
    "- **User**: user messages are the individual prompts that the user sends to the model\n",
    "- **Assistant**: assistant messages are the responses the model generates to the user's prompts\n",
    "\n",
    "If you're just chatting with ChatGPT via it's web-based User Interface (UI), you're probably familiar with **User** and **Assistant** messages, but you may not know that there's a **System** message behind the scenes that helps guide how the model responds to your messages.\n",
    "\n",
    "[Custom Instructions](https://openai.com/blog/custom-instructions-for-chatgpt) are sort of like [system prompts](https://github.com/jujumilk3/leaked-system-prompts), but don't give us quite as much control as we can exercise via the [Chat API](https://platform.openai.com/docs/api-reference/chat) - or as much control as we can get with some of the recent open source models.\n",
    "\n",
    "### Example conversation document\n",
    "\n",
    "When you put it all together, the whole thing looks a bit like this:\n",
    "\n",
    "```\n",
    "[System]\n",
    "Assistant is a large language model trained by OpenAI.\n",
    "Knowledge cutoff: 2021-09\n",
    "Current date: 2023-08-17\n",
    "Browsing: disabled\n",
    "\n",
    "[User]\n",
    "Who won the Super Bowl in 2022?\n",
    "\n",
    "[Assistant]\n",
    "I'm sorry, but I don't have access to real-time information as my knowledge\n",
    "was last updated in September 2021. To find out the winner of the Super Bowl\n",
    "in 2022, I recommend checking a reliable sports news website, a sports app,\n",
    "or conducting a quick internet search for the most up-to-date information.\n",
    "```\n",
    "\n",
    "**Note**: There are also messages with the type `function` that indicate that the Assistant would like to take advantage of [function calling](https://openai.com/blog/function-calling-and-other-api-updates) by asking the system to execute the function with the given name and pass it the given parameters, but for this demo, we'll be ignoring those.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8257c",
   "metadata": {},
   "source": [
    "### Basic example\n",
    "\n",
    "Here's an example of a basic prompt we could use for seniment analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c237a87",
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1694090931628,
     "user": {
      "displayName": "Matthias Kraft",
      "userId": "07587708083169164935"
     },
     "user_tz": -120
    },
    "id": "3c237a87"
   },
   "outputs": [],
   "source": [
    "BASIC_SYSTEM_PROMPT = \"\"\"\n",
    "You are VibeCheck, an advanced AI system for detecting the sentiment conveyed in user-generated text.\n",
    "\n",
    "The user will provide you with a prompt, and you will respond with the sentiment of that prompt.\n",
    "\n",
    "Do not include any punctuation and only use lower case letters.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def basicChatGptSentiment(prompt, model=modelDropdown.value):\n",
    "    messages = [{\"role\": \"system\", \"content\": BASIC_SYSTEM_PROMPT}]\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556a5d9b",
   "metadata": {},
   "source": [
    "Let's apply this to our Hacker News stories from earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_API_KEY:\n",
    "    for storyId, story in stories.items():\n",
    "        sentiment = basicChatGptSentiment(story[\"title\"])\n",
    "\n",
    "        if modelDropdown.value not in story[\"sentiment\"][\"openai\"]:\n",
    "            story[\"sentiment\"][\"openai\"][modelDropdown.value] = {}\n",
    "\n",
    "        story[\"sentiment\"][\"openai\"][modelDropdown.value][\"basic\"] = sentiment\n",
    "\n",
    "        print(f\"{story['title']} ({sentiment})\")\n",
    "else:\n",
    "    print(\"Please enter your OpenAI API key above and rerun this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f5236",
   "metadata": {},
   "source": [
    "### Going further\n",
    "\n",
    "What if we wanted to dig a bit deeper and consider the emotions that might be associated with some text rather than just a simple positive to negative spectrum?\n",
    "\n",
    "In the traditional NLP approach, there were tools like [NRCLex](https://pypi.org/project/nrclex/) that could help us with this, too.\n",
    "\n",
    "Let's explore how we could analyze the emotional content of some text with `nrclex`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bdb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrclex import NRCLex\n",
    "\n",
    "\n",
    "def getNRCEmotion(text):\n",
    "    emotion = NRCLex(text)\n",
    "\n",
    "    return emotion.top_emotions\n",
    "\n",
    "\n",
    "for storyId, story in stories.items():\n",
    "    emotions = []\n",
    "\n",
    "    emotionAnalysis = getNRCEmotion(story[\"title\"])\n",
    "\n",
    "    for emotion, value in emotionAnalysis:\n",
    "        if value > 0.00:\n",
    "            emotions.append(emotion)\n",
    "\n",
    "    story[\"sentiment\"][\"nrclex\"] = \", \".join(emotions)\n",
    "\n",
    "    print(\n",
    "        f\"{story['title']} {('(' + ', '.join(emotions) + ')') if len(emotions) else ''}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740845f9",
   "metadata": {},
   "source": [
    "But, with how short some of our titles can be, it doesn't always seem to get good results and it seems like sometimes it disagrees with the VADER sentiment analysis.\n",
    "\n",
    "Luckily, we can pretty easily adapt our initial prompt to get ChatGPT to do this for us, too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e479fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADVANCED_SYSTEM_PROMPT = \"\"\"\n",
    "You are VibeCheck, an advanced AI system for detecting the sentiment conveyed in user-generated text.\n",
    "\n",
    "The user will provide you with a prompt, and you will analyze it following these steps:\n",
    "\n",
    "1. Analyze the prompt for relevant emotion, tone, affinity, sarcasm, irony, etc.\n",
    "2. Analyze the likely emotional state of the author based on those findings\n",
    "3. Summarize the emotional state and sentiment of the prompt based on your findings with at least 2, but no more than 5 names for emotions\n",
    "\n",
    "Only return the output from the final step to the user.\n",
    "\n",
    "Only respond with lowercase letters and separate each emotion with a comma and a space\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def advancedChatGptSentiment(prompt, model=modelDropdown.value):\n",
    "    messages = [{\"role\": \"system\", \"content\": ADVANCED_SYSTEM_PROMPT}]\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a29a7",
   "metadata": {},
   "source": [
    "Let's apply this to our Hacker News stories from earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2aee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_API_KEY:\n",
    "    for storyId, story in stories.items():\n",
    "        sentiment = advancedChatGptSentiment(story[\"title\"])\n",
    "\n",
    "        if modelDropdown.value not in story[\"sentiment\"][\"openai\"]:\n",
    "            story[\"sentiment\"][\"openai\"][modelDropdown.value] = {}\n",
    "\n",
    "        story[\"sentiment\"][\"openai\"][modelDropdown.value][\"advanced\"] = sentiment\n",
    "\n",
    "        print(f\"{story['title']} ({sentiment})\")\n",
    "else:\n",
    "    print(\"Please enter your OpenAI API key above and rerun this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9046fad7",
   "metadata": {},
   "source": [
    "## Comparison of tools\n",
    "\n",
    "The widget below will allow you to enter arbitrary text and analyze it using the VADER sentiment analysis function from above, the NRCLex emotional analysis function from above, and the ChatGPT emotion analysis prompt we just created.\n",
    "\n",
    "Play around with it and see how our various tools respond.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59daa29b",
   "metadata": {
    "tags": [
     "hide-input",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "# this code cell is just used to display a widget\n",
    "# that uses the analyzeSentiment function we created\n",
    "# as well as the advancedChatGptSentiment function\n",
    "# you can just ignore/collapse it if you would prefer\n",
    "configureOpenAi(OPENAI_API_KEY, modelDropdown.value, TEMPERATURE)\n",
    "\n",
    "display(advancedAnalysisWidget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b05dc",
   "metadata": {},
   "source": [
    "## Beyond sentiment\n",
    "\n",
    "What if we were looking to do something a little more complicated than just basic sentiment or emotion analysis?\n",
    "\n",
    "What if we wanted to describe the sentiment of some text via an emoji?\n",
    "\n",
    "**Note**: GPT-4 seems to handle emojis better than GPT-3.5-Turbo, but will incur higher costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0058f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOJI_SYSTEM_PROMPT = \"\"\"\n",
    "You are VibeCheck, an advanced AI system for detecting the sentiment conveyed in user-generated text.\n",
    "\n",
    "The user will provide you with a prompt, and you will analyze it following these steps:\n",
    "\n",
    "1. Analyze the prompt for relevant emotion, tone, affinity, sarcasm, irony, etc.\n",
    "2. Analyze the likely emotional state of the author based on those findings\n",
    "3. Summarize the emotional state and sentiment of the prompt based on your findings with at least 2, but no more than 5 names for emotions\n",
    "4. Convert the emotional states from your findings into a representative emoji or group of emojis\n",
    "\n",
    "Only return the output from the final step to the user.\n",
    "\n",
    "Repsond with at least 1, but not more than 5, emoji.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def emojiChatGptSentiment(prompt, model=modelDropdown.value):\n",
    "    messages = [{\"role\": \"system\", \"content\": EMOJI_SYSTEM_PROMPT}]\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d0a395",
   "metadata": {},
   "source": [
    "Let's apply this to our Hacker News stories from earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f3196",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_API_KEY:\n",
    "    for storyId, story in stories.items():\n",
    "        sentiment = emojiChatGptSentiment(story[\"title\"])\n",
    "\n",
    "        if modelDropdown.value not in story[\"sentiment\"][\"openai\"]:\n",
    "            story[\"sentiment\"][\"openai\"][modelDropdown.value] = {}\n",
    "\n",
    "        story[\"sentiment\"][\"openai\"][modelDropdown.value][\"emoji\"] = sentiment\n",
    "\n",
    "        print(f\"{story['title']}({sentiment})\")\n",
    "else:\n",
    "    print(\"Please enter your OpenAI API key above and rerun this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda6a510",
   "metadata": {},
   "source": [
    "## Prompting strategies\n",
    "\n",
    "In the previous examples we've been using [Zero Shot](https://www.promptingguide.ai/techniques/zeroshot) prompting, which means we're asking the model to repsond without giving it an example of what kind of response we'd like for it to have.\n",
    "\n",
    "There are other prompting strategies we can employ, though:\n",
    "\n",
    "- **One Shot**: gives the model a single example of how we'd like it to respond to guide it's output; this is useful for situations where the model needs a little guidance, but we don't wnat to interfere with how it performs on other tasks\n",
    "- [**Few Shot**](https://www.promptingguide.ai/techniques/fewshot): gives the model a few examples of how we'd like it to respond to different prompts to help guide it's output; this is useful for situations where the model is doing something novel and needs more guidance, and we're going to be mostly focusing on asking the model to perform the task that we're providing examples for\n",
    "\n",
    "**Note**: For other types of tasks there are various prompting strategies that can be useful, like [Chain of Thought Reasoning](https://www.promptingguide.ai/techniques/cot), [Directional Stimulus Prompting](https://www.promptingguide.ai/techniques/dsp), and even telling the model to [take a deep breath](https://arstechnica.com/information-technology/2023/09/telling-ai-model-to-take-a-deep-breath-causes-math-scores-to-soar-in-study/) can help it do math.\n",
    "\n",
    "### Learn more about prompting strategies\n",
    "\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "- [Master Prompting Concepts: Zero-Shot and Few-Shot Prompting](https://www.promptengineering.org/master-prompting-concepts-zero-shot-and-few-shot-prompting/)\n",
    "- [ChatGPT Prompt Engineering Tips: Zero, One and Few Shot Prompting](https://www.allabtai.com/prompt-engineering-tips-zero-one-and-few-shot-prompting/)\n",
    "- [Tips to enhance your prompt-engineering abilities](https://cloud.google.com/blog/products/application-development/five-best-practices-for-prompt-engineering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc5cfb3",
   "metadata": {},
   "source": [
    "### One shot prompting\n",
    "\n",
    "Providing a single example of the desired output can help with things like proper formatting and refine the quality of the model's output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbed from https://news.ycombinator.com/ at 2023-09-20 13:00 EDT\n",
    "# Reference: https://news.ycombinator.com/item?id=37598299\n",
    "ONE_SHOT_USER_EXAMPLE = (\n",
    "    \"Cisco pulled out of the SentinelOne acquisition after due dilligence\"\n",
    ")\n",
    "\n",
    "ONE_SHOT_BOT_EXAMPLE = \"ü§®\"\n",
    "\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def oneShotChatGptSentiment(prompt, model=modelDropdown.value):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": EMOJI_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": ONE_SHOT_USER_EXAMPLE},\n",
    "        {\"role\": \"assistant\", \"content\": ONE_SHOT_BOT_EXAMPLE},\n",
    "    ]\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea18c0d",
   "metadata": {},
   "source": [
    "Let's apply this to our Hacker News stories from earlier and see how it changes the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb4d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_API_KEY:\n",
    "    for storyId, story in stories.items():\n",
    "        sentiment = oneShotChatGptSentiment(story[\"title\"])\n",
    "\n",
    "        if modelDropdown.value not in story[\"sentiment\"][\"openai\"]:\n",
    "            story[\"sentiment\"][\"openai\"][modelDropdown.value] = {}\n",
    "\n",
    "        story[\"sentiment\"][\"openai\"][modelDropdown.value][\"oneshot\"] = sentiment\n",
    "\n",
    "        print(f\"{story['title']}({sentiment})\")\n",
    "else:\n",
    "    print(\"Please enter your OpenAI API key above and rerun this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83570356",
   "metadata": {},
   "source": [
    "### Few shot prompting\n",
    "\n",
    "Providing a few examples of desired responses can give the model a chance to learn how you'd like it to respond.\n",
    "\n",
    "**Note**: Few shot prompting can also lead to issues where the model doesn't respond as creatively or won't perform as well on other tasks, which can be great for certain use cases, but might require a higher temperature setting for others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbed from https://news.ycombinator.com/ at 2023-09-20 13:10 EDT\n",
    "FEW_SHOT_USER_EXAMPLES = [\n",
    "    ONE_SHOT_USER_EXAMPLE,\n",
    "    # Reference: https://news.ycombinator.com/item?id=37595898\n",
    "    \"Atlassian cripples Jira automation for all but enterprise customers\",\n",
    "    # Reference: https://news.ycombinator.com/item?id=37586264\n",
    "    \"Toyota Research claims breakthrough in teaching robots new behaviors\",\n",
    "]\n",
    "\n",
    "FEW_SHOT_BOT_EXAMPLES = [\n",
    "    ONE_SHOT_BOT_EXAMPLE,\n",
    "    \"üòñ\",\n",
    "    \"üëè\",\n",
    "]\n",
    "\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def fewShotChatGptSentiment(prompt, model=modelDropdown.value):\n",
    "    messages = [{\"role\": \"system\", \"content\": EMOJI_SYSTEM_PROMPT}]\n",
    "\n",
    "    for i, userExample in enumerate(FEW_SHOT_USER_EXAMPLES):\n",
    "        messages.append({\"role\": \"user\", \"content\": userExample})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": FEW_SHOT_BOT_EXAMPLES[i]})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd690af",
   "metadata": {},
   "source": [
    "Let's apply this to our Hacker News stories from earlier and see how it changes the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67080489",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_API_KEY:\n",
    "    for storyId, story in stories.items():\n",
    "        sentiment = fewShotChatGptSentiment(story[\"title\"])\n",
    "\n",
    "        if modelDropdown.value not in story[\"sentiment\"][\"openai\"]:\n",
    "            story[\"sentiment\"][\"openai\"][modelDropdown.value] = {}\n",
    "\n",
    "        story[\"sentiment\"][\"openai\"][modelDropdown.value][\"fewshot\"] = sentiment\n",
    "\n",
    "        print(f\"{story['title']} ({sentiment})\")\n",
    "else:\n",
    "    print(\"Please enter your OpenAI API key above and rerun this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b4ceb9",
   "metadata": {},
   "source": [
    "## Comparing approaches\n",
    "\n",
    "We've looked at various approaches to analyzing sentiment and explored some interesting and novel ways that we can work with AI models like ChatGPT to perform tasks that used to require large investments of time to gather and label data and then train a model.\n",
    "\n",
    "Let's compare the results of each analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4023ba",
   "metadata": {},
   "source": [
    "### Gathering our data\n",
    "\n",
    "We'll start by mapping our data into a format that is easier to display with [DataFrames](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) provided by the [`pandas`](https://pandas.pydata.org/) library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e406deb6",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# this cell is used to gather our data into an object that's easier to work with\n",
    "# when displaying some dataframes with slices of what we've explored\n",
    "import pandas as pd\n",
    "\n",
    "sentimentData = {\n",
    "    \"Story\": [],\n",
    "    \"VADER\": [],\n",
    "    \"NRC\": [],\n",
    "    \"ChatGPT (Sentiment)\": [],\n",
    "    \"ChatGPT (Emotion)\": [],\n",
    "    \"Zero Shot\": [],\n",
    "    \"One Shot\": [],\n",
    "    \"Few Shot\": [],\n",
    "}\n",
    "\n",
    "for storyId, story in stories.items():\n",
    "    if \"title\" in story:\n",
    "        sentimentData[\"Story\"].append(story[\"title\"])\n",
    "\n",
    "    if \"vader\" in story[\"sentiment\"]:\n",
    "        sentimentData[\"VADER\"].append(story[\"sentiment\"][\"vader\"])\n",
    "\n",
    "    if \"nrclex\" in story[\"sentiment\"]:\n",
    "        sentimentData[\"NRC\"].append(story[\"sentiment\"][\"nrclex\"])\n",
    "\n",
    "    if (\n",
    "        \"openai\" in story[\"sentiment\"]\n",
    "        and modelDropdown.value in story[\"sentiment\"][\"openai\"]\n",
    "    ):\n",
    "        if \"basic\" in story[\"sentiment\"][\"openai\"][modelDropdown.value]:\n",
    "            sentimentData[\"ChatGPT (Sentiment)\"].append(\n",
    "                story[\"sentiment\"][\"openai\"][modelDropdown.value][\"basic\"]\n",
    "            )\n",
    "\n",
    "        if \"advanced\" in story[\"sentiment\"][\"openai\"][modelDropdown.value]:\n",
    "            sentimentData[\"ChatGPT (Emotion)\"].append(\n",
    "                story[\"sentiment\"][\"openai\"][modelDropdown.value][\"advanced\"]\n",
    "            )\n",
    "\n",
    "        if \"emoji\" in story[\"sentiment\"][\"openai\"][modelDropdown.value]:\n",
    "            sentimentData[\"Zero Shot\"].append(\n",
    "                story[\"sentiment\"][\"openai\"][modelDropdown.value][\"emoji\"]\n",
    "            )\n",
    "\n",
    "        if \"oneshot\" in story[\"sentiment\"][\"openai\"][modelDropdown.value]:\n",
    "            sentimentData[\"One Shot\"].append(\n",
    "                story[\"sentiment\"][\"openai\"][modelDropdown.value][\"oneshot\"]\n",
    "            )\n",
    "\n",
    "        if \"fewshot\" in story[\"sentiment\"][\"openai\"][modelDropdown.value]:\n",
    "            sentimentData[\"Few Shot\"].append(\n",
    "                story[\"sentiment\"][\"openai\"][modelDropdown.value][\"fewshot\"]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bdae02",
   "metadata": {},
   "source": [
    "### Sentiment analysis\n",
    "\n",
    "First let's compare the VADER sentiment analysis to our basic ChatGPT sentiment analysis prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2ddf1",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# this cell is only used to display a dataframe of our sentiment analysis results\n",
    "try:\n",
    "    if checkArrayLengths(\n",
    "        sentimentData[\"Story\"],\n",
    "        sentimentData[\"VADER\"],\n",
    "        sentimentData[\"ChatGPT (Sentiment)\"],\n",
    "    ):\n",
    "        sentimentDataFrame = pd.DataFrame(\n",
    "            data=sentimentData,\n",
    "            columns=[\"Story\", \"VADER\", \"ChatGPT (Sentiment)\"],\n",
    "        )\n",
    "\n",
    "        display(\n",
    "            sentimentDataFrame\n",
    "            if STORY_SAMPLE_SIZE <= DATAFRAME_LIMIT\n",
    "            else sentimentDataFrame.head(DATAFRAME_LIMIT)\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"Error: Different number of stories and sentiment results. Please rerun the VADER, Basic ChatGPT Example, and Gathering Our Data cells above and then rerun this cell.\"\n",
    "        )\n",
    "except NameError:\n",
    "    print(\n",
    "        \"Error: No sentiment data to display. Please rerun the Gathering Our Data cell above and then rerun this cell.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5f448a",
   "metadata": {},
   "source": [
    "### Emotion analysis\n",
    "\n",
    "Next let's compare the emotional analysis of NRCLex to our ChatGPT emotional analysis prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb674f0f",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# this code cell is only used to display a dataframe with our emotional analysis results\n",
    "try:\n",
    "    if checkArrayLengths(\n",
    "        sentimentData[\"Story\"], sentimentData[\"NRC\"], sentimentData[\"ChatGPT (Emotion)\"]\n",
    "    ):\n",
    "        emotionDataFrame = pd.DataFrame(\n",
    "            data=sentimentData, columns=[\"Story\", \"NRC\", \"ChatGPT (Emotion)\"]\n",
    "        )\n",
    "\n",
    "        # often NRCLex will not have data and instead of displaying NaN we'll leave it blank\n",
    "        emotionDataFrame = emotionDataFrame.fillna(\"\")\n",
    "\n",
    "        display(\n",
    "            emotionDataFrame\n",
    "            if STORY_SAMPLE_SIZE <= DATAFRAME_LIMIT\n",
    "            else emotionDataFrame.head(DATAFRAME_LIMIT)\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"Error: Different number of stories and sentiment results. Please rerun the NRCLex, Advanced ChatGPT Example, and Gathering Our Data cells above and then rerun this cell.\"\n",
    "        )\n",
    "except NameError:\n",
    "    print(\n",
    "        \"Error: No emotion data to display. Please rerun the Gathering Our Data cell above and then rerun this cell.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2438c356",
   "metadata": {},
   "source": [
    "### Prompting strategies\n",
    "\n",
    "Finally, let's compare the zero shot, one shot, and few shot approaches to our emoji analyzer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d8160",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# this cell is just used to display a dataframe with our emoji results\n",
    "try:\n",
    "    if checkArrayLengths(\n",
    "        sentimentData[\"Story\"],\n",
    "        sentimentData[\"Zero Shot\"],\n",
    "        sentimentData[\"One Shot\"],\n",
    "        sentimentData[\"Few Shot\"],\n",
    "    ):\n",
    "        emojiDataFrame = pd.DataFrame(\n",
    "            data=sentimentData, columns=[\"Story\", \"Zero Shot\", \"One Shot\", \"Few Shot\"]\n",
    "        )\n",
    "\n",
    "        display(\n",
    "            emojiDataFrame\n",
    "            if STORY_SAMPLE_SIZE <= DATAFRAME_LIMIT\n",
    "            else emojiDataFrame.head(DATAFRAME_LIMIT)\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"Error: Different number of stories and emoji results. Please rerun the Emjoji Classifier, One Shot, Few Shot, and Gathering Our Data cells above and then rerun this cell.\"\n",
    "        )\n",
    "except NameError:\n",
    "    print(\n",
    "        \"Error: No emoji data to display. Please rerun the Gathering Our Data cell above and then rerun this cell.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17221601",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "NLP tasks like sentiment analyis used to required significant resources and time, but with the advent of LLMs like ChatGPT and the continued discovery of new prompting strategies to guide these models we can quickly perform complex NLP analyses and teach models to perform novel tasks.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
